{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PINN code for Porous Media Permeability Upscaling - Predict L1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Libraries\n",
    "from numpy import sqrt\n",
    "from numpy import savetxt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import os\n",
    "# ML libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.python.keras.utils import losses_utils\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "from contextlib import redirect_stdout\n",
    "# Visualization libraries\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "# generates same random numbers each time\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "# Choose GPU to use (8 available) - skip 0 (busy)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_visible_devices(gpus[4:8], 'GPU')\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Set data type\n",
    "DTYPE='float32'\n",
    "tf.keras.backend.set_floatx(DTYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data - Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data filename\n",
    "rock_type = \"BrSS\"\n",
    "data_file = './data/'+rock_type+'_compiled_data_level_1'   \n",
    "# Save Folder\n",
    "mode = 'loss'                                                             #update, loss, lambda                   \n",
    "formula = 'elmorsy'\n",
    "trial = '_'+rock_type+'_level_1_'+formula+'_'+mode+'_1'                   #update\n",
    "ana_sol = \"hat_\"+formula\n",
    "#\n",
    "if mode == 'loss':\n",
    "    network = 'mlp_data_split_0.2_3_rocks_KF_elmorsy_8x4L_BEST'\n",
    "if mode =='lambda':\n",
    "    network = 'mlp_data_split_0.2_3_rocks_KF_elmorsy_Lambda_test_7_BEST'\n",
    "#\n",
    "save_name = './networks/predict/'+network+trial\n",
    "# Load data files\n",
    "data = pd.read_csv(data_file+\".csv\")\n",
    "# List data heads\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Custome Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_XY(data):\n",
    "    X = pd.DataFrame()\n",
    "    for i in range(1, 9):\n",
    "        kx = \"kx\"+str(i)\n",
    "        ky = \"ky\"+str(i)\n",
    "        kz = \"kz\"+str(i)\n",
    "        X = pd.concat([X, data[kx], data[ky], data[kz]], axis=1)\n",
    "    Y = data[\"KX\"]  \n",
    "\n",
    "    return X, Y\n",
    "\n",
    "def data_rotate(data):\n",
    "    data_len = len(data[\"KX\"])\n",
    "    # create extended data\n",
    "    column_list = list(data)\n",
    "    zero_data = np.zeros(shape=(len(data),len(column_list)))\n",
    "    zero_data = pd.DataFrame(zero_data, columns=column_list)\n",
    "    data_ext = pd.concat([data, zero_data, zero_data], ignore_index=True)\n",
    "    # Rotate and combine data\n",
    "    # large K dataset\n",
    "    data_ext['KX'] = pd.concat([data['KX'], data['KY'], data['KZ']], ignore_index=True)\n",
    "    # small k in X Directions\n",
    "    data_ext['kx1'] = pd.concat([data['kx1'], data['ky1'], data['kz1']], ignore_index=True)\n",
    "    data_ext['kx2'] = pd.concat([data['kx2'], data['ky5'], data['kz2']], ignore_index=True)\n",
    "    data_ext['kx3'] = pd.concat([data['kx3'], data['ky3'], data['kz5']], ignore_index=True)\n",
    "    data_ext['kx4'] = pd.concat([data['kx4'], data['ky7'], data['kz6']], ignore_index=True)\n",
    "    data_ext['kx5'] = pd.concat([data['kx5'], data['ky2'], data['kz3']], ignore_index=True)\n",
    "    data_ext['kx6'] = pd.concat([data['kx6'], data['ky6'], data['kz4']], ignore_index=True)\n",
    "    data_ext['kx7'] = pd.concat([data['kx7'], data['ky4'], data['kz7']], ignore_index=True)\n",
    "    data_ext['kx8'] = pd.concat([data['kx8'], data['ky8'], data['kz8']], ignore_index=True)\n",
    "    # small k in Y Directions\n",
    "    data_ext['ky1'] = pd.concat([data['ky1'], data['kx1'], data['ky1']], ignore_index=True)\n",
    "    data_ext['ky2'] = pd.concat([data['ky2'], data['kx5'], data['ky2']], ignore_index=True)\n",
    "    data_ext['ky3'] = pd.concat([data['ky3'], data['kx3'], data['ky5']], ignore_index=True)\n",
    "    data_ext['ky4'] = pd.concat([data['ky4'], data['kx7'], data['ky6']], ignore_index=True)\n",
    "    data_ext['ky5'] = pd.concat([data['ky5'], data['kx2'], data['ky3']], ignore_index=True)\n",
    "    data_ext['ky6'] = pd.concat([data['ky6'], data['kx6'], data['ky4']], ignore_index=True)\n",
    "    data_ext['ky7'] = pd.concat([data['ky7'], data['kx4'], data['ky7']], ignore_index=True)\n",
    "    data_ext['ky8'] = pd.concat([data['ky8'], data['kx8'], data['ky8']], ignore_index=True)\n",
    "    # small k in Z Directions\n",
    "    data_ext['kz1'] = pd.concat([data['kz1'], data['kz1'], data['kx1']], ignore_index=True)\n",
    "    data_ext['kz2'] = pd.concat([data['kz2'], data['kz5'], data['kx2']], ignore_index=True)\n",
    "    data_ext['kz3'] = pd.concat([data['kz3'], data['kz3'], data['kx5']], ignore_index=True)\n",
    "    data_ext['kz4'] = pd.concat([data['kz4'], data['kz7'], data['kx6']], ignore_index=True)\n",
    "    data_ext['kz5'] = pd.concat([data['kz5'], data['kz2'], data['kx3']], ignore_index=True)\n",
    "    data_ext['kz6'] = pd.concat([data['kz6'], data['kz6'], data['kx4']], ignore_index=True)\n",
    "    data_ext['kz7'] = pd.concat([data['kz7'], data['kz4'], data['kx7']], ignore_index=True)\n",
    "    data_ext['kz8'] = pd.concat([data['kz8'], data['kz8'], data['kx8']], ignore_index=True)\n",
    "    # porosity\n",
    "    data_ext['P'] = pd.concat([data['P'], data['P'], data['P']], ignore_index=True)\n",
    "    data_ext['p1'] = pd.concat([data['p1'], data['p1'], data['p1']], ignore_index=True)\n",
    "    data_ext['p2'] = pd.concat([data['p2'], data['p5'], data['p2']], ignore_index=True)\n",
    "    data_ext['p3'] = pd.concat([data['p3'], data['p3'], data['p5']], ignore_index=True)\n",
    "    data_ext['p4'] = pd.concat([data['p4'], data['p7'], data['p6']], ignore_index=True)\n",
    "    data_ext['p5'] = pd.concat([data['p5'], data['p2'], data['p3']], ignore_index=True)\n",
    "    data_ext['p6'] = pd.concat([data['p6'], data['p6'], data['p4']], ignore_index=True)\n",
    "    data_ext['p7'] = pd.concat([data['p7'], data['p4'], data['p7']], ignore_index=True)\n",
    "    data_ext['p8'] = pd.concat([data['p8'], data['p8'], data['p8']], ignore_index=True)\n",
    "    # eff. porosity\n",
    "    data_ext['EP'] = pd.concat([data['EP'], data['EP'], data['EP']], ignore_index=True)\n",
    "    data_ext['ep1'] = pd.concat([data['ep1'], data['ep1'], data['ep1']], ignore_index=True)\n",
    "    data_ext['ep2'] = pd.concat([data['ep2'], data['ep5'], data['ep2']], ignore_index=True)\n",
    "    data_ext['ep3'] = pd.concat([data['ep3'], data['ep3'], data['ep5']], ignore_index=True)\n",
    "    data_ext['ep4'] = pd.concat([data['ep4'], data['ep7'], data['ep6']], ignore_index=True)\n",
    "    data_ext['ep5'] = pd.concat([data['ep5'], data['ep2'], data['ep3']], ignore_index=True)\n",
    "    data_ext['ep6'] = pd.concat([data['ep6'], data['ep6'], data['ep4']], ignore_index=True)\n",
    "    data_ext['ep7'] = pd.concat([data['ep7'], data['ep4'], data['ep7']], ignore_index=True)\n",
    "    data_ext['ep8'] = pd.concat([data['ep8'], data['ep8'], data['ep8']], ignore_index=True)\n",
    "    # case-name\n",
    "    data_ext['case-name'] = pd.concat([data['case-name'], data['case-name'], data['case-name']], ignore_index=True)\n",
    "    # add directions\n",
    "    data_ext.rename(columns = {'KY':'direction'}, inplace = True)\n",
    "    ones = pd.Series(np.ones(data_len))\n",
    "    data_ext['direction'] = pd.concat([ones, ones*2, ones*3], ignore_index=True)\n",
    "    # drop remaining features\n",
    "    data_ext = data_ext.drop(columns=['KZ', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8'])\n",
    "    \n",
    "    return data_ext\n",
    "\n",
    "def data_summary(hat, true, percentile):\n",
    "    # calc error\n",
    "    RE = (hat-true)/true\n",
    "    ARE = RE.abs()    \n",
    "    # Summarze data in one dataframe\n",
    "    data_summary = pd.DataFrame(columns=['hat', 'true', 'ARE'], index=range(len(true)))\n",
    "    data_summary['true'] = true.reset_index(drop=True)\n",
    "    data_summary['hat'] = hat\n",
    "    data_summary['ARE'] = ARE.reset_index(drop=True)    \n",
    "    # 95 percintile of Testing data results\n",
    "    data_summary = data_summary[(data_summary.ARE < ARE.quantile(percentile))]\n",
    "    \n",
    "    return data_summary\n",
    "    \n",
    "def print_results(MARE_ana, MARE, MedRE, ER):\n",
    "    print('Testing MARE  (Analytical) [%] = '+str(round(MARE_ana*100, 1)))\n",
    "    print('Testing MARE  (PINN)       [%] = '+str(round(MARE*100, 1)))\n",
    "    print('Testing MedRE (PINN)       [%] = '+str(round(MedRE*100, 1)))\n",
    "    print('MARE Reduction             [%] = '+str(round(ER*100, 1)))\n",
    "    return\n",
    "\n",
    "# Visualization\n",
    "def hist(x, bins=30, color='lightgreen', alpha=1, xlabel='xlabel', title='title',\n",
    "         stat=None, xlim=None, ylim=None, version=''):\n",
    "    MARE = np.mean(abs(x))\n",
    "    MedRE = np.median(abs(x))\n",
    "    plt.hist(x, bins=bins, color=color, alpha=alpha, edgecolor='black', linewidth=0.7)\n",
    "    \n",
    "    plt.tick_params(direction=\"in\", pad=5, labelsize=12)\n",
    "    if ylim != None:\n",
    "        plt.ylim(ylim[0],ylim[1])\n",
    "    if xlim != None:\n",
    "        plt.xlim(xlim[0],xlim[1])\n",
    "    if stat == False:\n",
    "        plt.title(title, size=12)\n",
    "    else:\n",
    "        MARE = np.mean(abs(x))\n",
    "        MedRE = np.median(abs(x))\n",
    "        plt.axvline(MARE, color='r', linestyle='dashed', linewidth=1)\n",
    "        plt.axvline(MedRE, color='k', linestyle='dashed', linewidth=1)\n",
    "        plt.legend(['Mean', 'Median'], loc='upper right')\n",
    "        plt.title(title+'\\n MARE= '+str(round(MARE, 2)), size=12)\n",
    "        # plt.title(title, size=12)\n",
    "    plt.ylabel('Number of samples [-]', size=12)\n",
    "    plt.xlabel(xlabel, size=12)\n",
    "    plt.savefig(color+version+'_hist.png', dpi=300)\n",
    "    plt.savefig(color+version+'_hist.pdf', dpi=300)\n",
    "    #plt.savefig('K_hist.eps', format='eps')\n",
    "    plt.show()\n",
    "    #plt.clf()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace zeros\n",
    "data_1 = data.replace(0, 0.000001)\n",
    "# Rotate data\n",
    "data_3dir = data_rotate(data_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select Input and Target data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = get_XY(data_3dir)\n",
    "Y_test_true = Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PINN Custome Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Lambda Function - 3D\n",
    "def keff_3d_aniso_tensor(tensor):\n",
    "    # print(tensor)\n",
    "    # k1\n",
    "    kx1 = tensor[:, 0:1]\n",
    "    ky1 = tensor[:, 1:2]\n",
    "    kz1 = tensor[:, 2:3]\n",
    "    # print(kx1)\n",
    "    # k2\n",
    "    kx2 = tensor[:, 3:4]\n",
    "    ky2 = tensor[:, 4:5]\n",
    "    kz2 = tensor[:, 5:6]\n",
    "    # k3\n",
    "    kx3 = tensor[:, 6:7]\n",
    "    ky3 = tensor[:, 7:8]\n",
    "    kz3 = tensor[:, 8:9]\n",
    "    # k4\n",
    "    kx4 = tensor[:, 9:10]\n",
    "    ky4 = tensor[:, 10:11]\n",
    "    kz4 = tensor[:, 11:12]\n",
    "    # k5\n",
    "    kx5 = tensor[:, 12:13]\n",
    "    ky5 = tensor[:, 13:14]\n",
    "    kz5 = tensor[:, 14:15]\n",
    "    # k6\n",
    "    kx6 = tensor[:, 15:16]\n",
    "    ky6 = tensor[:, 16:17]\n",
    "    kz6 = tensor[:, 17:18]\n",
    "    # k7\n",
    "    kx7 = tensor[:, 18:19]\n",
    "    ky7 = tensor[:, 19:20]\n",
    "    kz7 = tensor[:, 20:21]\n",
    "    # k8\n",
    "    kx8 = tensor[:, 21:22]\n",
    "    ky8 = tensor[:, 22:23]\n",
    "    kz8 = tensor[:, 23:24]\n",
    "\n",
    "    def keff_2d_aniso(k1x, k2x, k3x, k4x, k1z, k2z, k3z, k4z):\n",
    "     \n",
    "        keff_2d = 1/(k1z*k3z/(2*(k1x*k1z*k3x + k1x*k1z*k3z + k1x*k3x*k3z + k1z*k3x*k3z)) + \n",
    "                k2z*k4z/(2*(k2x*k2z*k4x + k2x*k2z*k4z + k2x*k4x*k4z + k2z*k4x*k4z)) + \n",
    "                (k1x*(k1z + k3z)/(2*(k1x*k1z*k3x + k1x*k1z*k3z + k1x*k3x*k3z + k1z*k3x*k3z)) + \n",
    "                k2x*(k2z + k4z)/(2*(k2x*k2z*k4x + k2x*k2z*k4z + k2x*k4x*k4z + k2z*k4x*k4z)) + \n",
    "                1/(2*k4x) + 1/(2*k3x))*(k3x*(k1z + k3z)/(2*(k1x*k1z*k3x + k1x*k1z*k3z + k1x*k3x*k3z + k1z*k3x*k3z)) + \n",
    "                k4x*(k2z + k4z)/(2*(k2x*k2z*k4x + k2x*k2z*k4z + k2x*k4x*k4z + k2z*k4x*k4z)) + \n",
    "                1/(2*k2x) + 1/(2*k1x))/(k1x*(k1z + k3z)/(2*(k1x*k1z*k3x + k1x*k1z*k3z + k1x*k3x*k3z + k1z*k3x*k3z)) + \n",
    "                k2x*(k2z + k4z)/(2*(k2x*k2z*k4x + k2x*k2z*k4z + k2x*k4x*k4z + k2z*k4x*k4z)) + \n",
    "                k3x*(k1z + k3z)/(2*(k1x*k1z*k3x + k1x*k1z*k3z + k1x*k3x*k3z + k1z*k3x*k3z)) + \n",
    "                k4x*(k2z + k4z)/(2*(k2x*k2z*k4x + k2x*k2z*k4z + k2x*k4x*k4z + k2z*k4x*k4z)) + \n",
    "                1/(2*k4x) + 1/(2*k3x) + 1/(2*k2x) + 1/(2*k1x)))\n",
    "        \n",
    "        return keff_2d\n",
    "    \n",
    "    # Left face > casenames: 1-5-3-7 (kx, kz)\n",
    "    k_2d_l = keff_2d_aniso(kx1, kx5, kx3, kx7, kz1, kz5, kz3, kz7)\n",
    "    \n",
    "    # Right face > casenames: 2-6-4-8 (kx, kz)\n",
    "    k_2d_r = keff_2d_aniso(kx2, kx6, kx4, kx8, kz2, kz6, kz4, kz8)\n",
    "    \n",
    "    # Top face > casenames: 1-5-2-6 (kx, ky)\n",
    "    k_2d_t = keff_2d_aniso(kx1, kx5, kx2, kx6, ky1, ky5, ky2, ky6)\n",
    "    \n",
    "    # Bottom face > casenames: 3-7-4-8 (kx, ky)\n",
    "    k_2d_b = keff_2d_aniso(kx3, kx7, kx4, kx8, ky3, ky7, ky4, ky8)\n",
    "    \n",
    "    # Calculate Keff in 3D\n",
    "    keff_3d = (k_2d_l + k_2d_r + k_2d_t + k_2d_b) / 4\n",
    "    \n",
    "    return keff_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Lambda Functions - 2D\n",
    "def keff_2d_aniso(k1x, k2x, k3x, k4x, k1z, k2z, k3z, k4z):\n",
    "    \n",
    "    keff_2d = 1/(k1z*k3z/(2*(k1x*k1z*k3x + k1x*k1z*k3z + k1x*k3x*k3z + k1z*k3x*k3z)) + \n",
    "            k2z*k4z/(2*(k2x*k2z*k4x + k2x*k2z*k4z + k2x*k4x*k4z + k2z*k4x*k4z)) + \n",
    "            (k1x*(k1z + k3z)/(2*(k1x*k1z*k3x + k1x*k1z*k3z + k1x*k3x*k3z + k1z*k3x*k3z)) + \n",
    "            k2x*(k2z + k4z)/(2*(k2x*k2z*k4x + k2x*k2z*k4z + k2x*k4x*k4z + k2z*k4x*k4z)) + \n",
    "            1/(2*k4x) + 1/(2*k3x))*(k3x*(k1z + k3z)/(2*(k1x*k1z*k3x + k1x*k1z*k3z + k1x*k3x*k3z + k1z*k3x*k3z)) + \n",
    "            k4x*(k2z + k4z)/(2*(k2x*k2z*k4x + k2x*k2z*k4z + k2x*k4x*k4z + k2z*k4x*k4z)) + \n",
    "            1/(2*k2x) + 1/(2*k1x))/(k1x*(k1z + k3z)/(2*(k1x*k1z*k3x + k1x*k1z*k3z + k1x*k3x*k3z + k1z*k3x*k3z)) + \n",
    "            k2x*(k2z + k4z)/(2*(k2x*k2z*k4x + k2x*k2z*k4z + k2x*k4x*k4z + k2z*k4x*k4z)) + \n",
    "            k3x*(k1z + k3z)/(2*(k1x*k1z*k3x + k1x*k1z*k3z + k1x*k3x*k3z + k1z*k3x*k3z)) + \n",
    "            k4x*(k2z + k4z)/(2*(k2x*k2z*k4x + k2x*k2z*k4z + k2x*k4x*k4z + k2z*k4x*k4z)) + \n",
    "            1/(2*k4x) + 1/(2*k3x) + 1/(2*k2x) + 1/(2*k1x)))\n",
    "    \n",
    "    return keff_2d\n",
    "\n",
    "def keff_2d_aniso_tensor_l(tensor):\n",
    "    # k1\n",
    "    kx1 = tensor[:, 0:1]\n",
    "    kz1 = tensor[:, 2:3]\n",
    "    # k3\n",
    "    kx3 = tensor[:, 6:7]\n",
    "    kz3 = tensor[:, 8:9]\n",
    "    # k5\n",
    "    kx5 = tensor[:, 12:13]\n",
    "    kz5 = tensor[:, 14:15]\n",
    "    # k7\n",
    "    kx7 = tensor[:, 18:19]\n",
    "    kz7 = tensor[:, 20:21]\n",
    "    \n",
    "    # Left face > casenames: 1-5-3-7 (kx, kz)\n",
    "    k_2d_l = keff_2d_aniso(kx1, kx5, kx3, kx7, kz1, kz5, kz3, kz7)\n",
    "    \n",
    "    return k_2d_l\n",
    "\n",
    "def keff_2d_aniso_tensor_r(tensor):\n",
    "    # k2\n",
    "    kx2 = tensor[:, 3:4]\n",
    "    kz2 = tensor[:, 5:6]\n",
    "    # k4\n",
    "    kx4 = tensor[:, 9:10]\n",
    "    kz4 = tensor[:, 11:12]\n",
    "    # k6\n",
    "    kx6 = tensor[:, 15:16]\n",
    "    kz6 = tensor[:, 17:18]\n",
    "    # k8\n",
    "    kx8 = tensor[:, 21:22]\n",
    "    kz8 = tensor[:, 23:24]\n",
    "\n",
    "    # Right face > casenames: 2-6-4-8 (kx, kz)\n",
    "    k_2d_r = keff_2d_aniso(kx2, kx6, kx4, kx8, kz2, kz6, kz4, kz8)\n",
    "    \n",
    "    return k_2d_r\n",
    "\n",
    "def keff_2d_aniso_tensor_t(tensor):\n",
    "    # print(tensor)\n",
    "    # k1\n",
    "    kx1 = tensor[:, 0:1]\n",
    "    ky1 = tensor[:, 1:2]\n",
    "    # print(kx1)\n",
    "    # k2\n",
    "    kx2 = tensor[:, 3:4]\n",
    "    ky2 = tensor[:, 4:5]\n",
    "    # k5\n",
    "    kx5 = tensor[:, 12:13]\n",
    "    ky5 = tensor[:, 13:14]\n",
    "    # k6\n",
    "    kx6 = tensor[:, 15:16]\n",
    "    ky6 = tensor[:, 16:17]\n",
    "    \n",
    "    # Top face > casenames: 1-5-2-6 (kx, ky)\n",
    "    k_2d_t = keff_2d_aniso(kx1, kx5, kx2, kx6, ky1, ky5, ky2, ky6)\n",
    "\n",
    "    return k_2d_t\n",
    "\n",
    "def keff_2d_aniso_tensor_b(tensor):\n",
    "    # k3\n",
    "    kx3 = tensor[:, 6:7]\n",
    "    ky3 = tensor[:, 7:8]\n",
    "    # k4\n",
    "    kx4 = tensor[:, 9:10]\n",
    "    ky4 = tensor[:, 10:11]\n",
    "    # k7\n",
    "    kx7 = tensor[:, 18:19]\n",
    "    ky7 = tensor[:, 19:20]\n",
    "    # k8\n",
    "    kx8 = tensor[:, 21:22]\n",
    "    ky8 = tensor[:, 22:23]\n",
    "\n",
    "    # Bottom face > casenames: 3-7-4-8 (kx, ky)\n",
    "    k_2d_b = keff_2d_aniso(kx3, kx7, kx4, kx8, ky3, ky7, ky4, ky8)\n",
    "    \n",
    "    return k_2d_b\n",
    "\n",
    "\n",
    "def keff_3d_aniso(kx1, kx2, kx3, kx4, kx5, kx6, kx7, kx8,\n",
    "                  ky1, ky2, ky3, ky4, ky5, ky6, ky7, ky8,\n",
    "                  kz1, kz2, kz3, kz4, kz5, kz6, kz7, kz8):\n",
    "      \n",
    "    # Left face > casenames: 1-5-3-7 (kx, kz)\n",
    "    k_2d_l = keff_2d_aniso(kx1, kx5, kx3, kx7, kz1, kz5, kz3, kz7)\n",
    "    \n",
    "    # Right face > casenames: 2-6-4-8 (kx, kz)\n",
    "    k_2d_r = keff_2d_aniso(kx2, kx6, kx4, kx8, kz2, kz6, kz4, kz8)\n",
    "    \n",
    "    # Top face > casenames: 1-5-2-6 (kx, ky)\n",
    "    k_2d_t = keff_2d_aniso(kx1, kx5, kx2, kx6, ky1, ky5, ky2, ky6)\n",
    "    \n",
    "    # Bottom face > casenames: 3-7-4-8 (kx, ky)\n",
    "    k_2d_b = keff_2d_aniso(kx3, kx7, kx4, kx8, ky3, ky7, ky4, ky8)\n",
    "    \n",
    "    # Calculate Keff in 3D\n",
    "    keff_3d = (k_2d_l + k_2d_r + k_2d_t + k_2d_b) / 4\n",
    "    \n",
    "    return keff_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PINN Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model_loss():\n",
    "    # Initialize a feedforward neural network\n",
    "    n_features = X_test.shape[1]\n",
    "    inputs = keras.Input(shape=(n_features))\n",
    "    x = layers.Dense((8), activation='relu', name='1st_layer')(inputs)  # 'name=' is optional\n",
    "    x = layers.Dense((8), activation='relu', name='2nd_layer')(x)\n",
    "    x = layers.Dense((8), activation='relu', name='3nd_layer')(x)\n",
    "    x = layers.Dense((8), activation='relu', name='4th_layer')(x)\n",
    "    outputs = layers.Dense((1))(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def init_model_lambda():\n",
    "    # Initialize a feedforward neural network\n",
    "    n_features = X_test.shape[1]\n",
    "    inputs = keras.Input(shape=(n_features))\n",
    "    x = layers.Dense((4), activation='relu', name='1st_layer')(inputs)  # 'name=' is optional\n",
    "    x = layers.Dense((4), activation='relu', name='2nd_layer')(x)\n",
    "    x = layers.Dense((4), activation='relu', name='3rd_layer')(x)\n",
    "    x = layers.Dense((4), activation='relu', name='4th_layer')(x)\n",
    "    #\n",
    "    y0 = layers.Lambda(keff_3d_aniso_tensor, name=\"lambda_layer_3d\")(inputs)\n",
    "    y1 = layers.Lambda(keff_2d_aniso_tensor_l, name=\"lambda_layer_2d_l\")(inputs)\n",
    "    y2 = layers.Lambda(keff_2d_aniso_tensor_r, name=\"lambda_layer_2d_r\")(inputs)\n",
    "    y3 = layers.Lambda(keff_2d_aniso_tensor_t, name=\"lambda_layer_2d_t\")(inputs)\n",
    "    y4 = layers.Lambda(keff_2d_aniso_tensor_b, name=\"lambda_layer_2d_b\")(inputs)\n",
    "    #\n",
    "    y = layers.concatenate([y0, y1, y2, y3, y4], axis=-1)\n",
    "    #\n",
    "    y = layers.Dense((4), activation='relu', name='2th_layer_lambda')(y)\n",
    "    y = layers.Dense((4), activation='relu', name='3th_layer_lambda')(y)\n",
    "    y = layers.Dense((4), activation='relu', name='4th_layer_lambda')(y)\n",
    "    #\n",
    "    x = layers.concatenate([x, y], axis=-1)\n",
    "    #\n",
    "    x = layers.Dense((4), activation='relu', name='1st_FCL_layer')(x)\n",
    "    x = layers.Dense((4), activation='relu', name='2nd_FCL_layer')(x)\n",
    "    #\n",
    "    outputs = layers.Dense((1))(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "# Loss Function\n",
    "loss = tf.keras.losses.MeanAbsolutePercentageError(\n",
    "        reduction=losses_utils.ReductionV2.AUTO, name='mean_absolute_percentage_error')\n",
    "acc_metric = tf.keras.metrics.MeanAbsolutePercentageError()\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "# Using Multiple GPUs takes more time than using one GPU.\n",
    "#\n",
    "if mode == 'loss':\n",
    "    model = init_model_loss()\n",
    "if mode =='lambda':\n",
    "    model = init_model_lambda()\n",
    "#      \n",
    "# Compile Model\n",
    "model.compile(loss=loss, \n",
    "                optimizer=optimizer, \n",
    "                metrics=[acc_metric])\n",
    "\n",
    "# Create Diectory\n",
    "isExist = os.path.exists(save_name)\n",
    "if not isExist:\n",
    "     os.mkdir(save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load weights\n",
    "checkpoint_filepath = './networks/'+network+'/checkpoint'\n",
    "model.load_weights(checkpoint_filepath)\n",
    "\n",
    "with open(save_name+'/model_summary.txt', 'w') as f:\n",
    "    with redirect_stdout(f):\n",
    "        model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Predictions and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions - PINN \n",
    "Y_test_pinn = tf.squeeze(model.predict(X_test)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions - Elmorsy\n",
    "data_0 = data\n",
    "data = data_3dir\n",
    "Y_test_elmo = pd.Series(np.zeros(len(data['KX'])))\n",
    "for i in range(0, len(data['KX'])):\n",
    "    Y_test_elmo[i] = keff_3d_aniso(data['kx1'][i], data['kx2'][i], data['kx3'][i], data['kx4'][i], \n",
    "                                   data['kx5'][i], data['kx6'][i], data['kx7'][i], data['kx8'][i],\n",
    "                             \n",
    "                                   data['ky1'][i], data['ky2'][i], data['ky3'][i], data['ky4'][i], \n",
    "                                   data['ky5'][i], data['ky6'][i], data['ky7'][i], data['ky8'][i],\n",
    "                             \n",
    "                                   data['kz1'][i], data['kz2'][i], data['kz3'][i], data['kz4'][i], \n",
    "                                   data['kz5'][i], data['kz6'][i], data['kz7'][i], data['kz8'][i])\n",
    "\n",
    "data = data_0\n",
    "del data_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting 3 directions into 1 direction\n",
    "data_3dir[\"KX_pinn\"] = Y_test_pinn\n",
    "data_3dir[\"KX_elmo\"] = Y_test_elmo\n",
    "data_hat = data_3dir[[\"case-name\", \"direction\", \"P\", \"EP\", \"KX\", \"KX_pinn\", \"KX_elmo\"]]\n",
    "#\n",
    "#\n",
    "data_hat_y = data_hat.loc[data_hat[\"direction\"] == 2].reset_index()\n",
    "data_hat_z = data_hat.loc[data_hat[\"direction\"] == 3].reset_index()\n",
    "data_hat = data_hat.loc[data_hat[\"direction\"] == 1].reset_index()\n",
    "#\n",
    "data_hat[[\"KY_pinn\", \"KY_elmo\", \"case-name_2\"]] = data_hat_y[[\"KX_pinn\", \"KX_elmo\", \"case-name\"]]\n",
    "data_hat[[\"KZ_pinn\", \"KZ_elmo\", \"case-name_3\"]] = data_hat_z[[\"KX_pinn\", \"KX_elmo\", \"case-name\"]]\n",
    "#\n",
    "d1 = np.sum(data_hat[\"case-name\"] - data_hat[\"case-name_2\"])\n",
    "d2 = np.sum(data_hat[\"case-name\"] - data_hat[\"case-name_3\"])\n",
    "d3 = np.sum(data_hat[\"case-name_2\"] - data_hat[\"case-name_3\"])\n",
    "print(\"case-names sum check: \"+ str(d1+d2+d3))\n",
    "#\n",
    "data_hat = data_hat.drop(columns=[\"direction\", \"case-name_2\", \"case-name_3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"case-name\", \"Porosity\", \"Eff. Porosity\", \"K(x)[D]\", \"K(y)[D]\", \"K(z)[D]\", \"M. time[s]\", \"S. Time[s]\"]\n",
    "numRows = data_hat.shape[0] \n",
    "data_kxyz_pinn = pd.DataFrame(columns=cols, index=range(numRows))\n",
    "data_kxyz_elmo = pd.DataFrame(columns=cols, index=range(numRows))\n",
    "#\n",
    "data_kxyz_pinn[cols] = data_hat[[\"case-name\", \"P\", \"EP\", \"KX_pinn\", \"KY_pinn\", \"KZ_pinn\", \"case-name\", \"case-name\"]]\n",
    "data_kxyz_elmo[cols] = data_hat[[\"case-name\", \"P\", \"EP\", \"KX_elmo\", \"KY_elmo\", \"KZ_elmo\", \"case-name\", \"case-name\"]]\n",
    "#\n",
    "# Save files\n",
    "data_kxyz_pinn.to_csv(save_name+'/'+rock_type+'_300_kxyz_results_pinn_'+mode+'.csv', index=False)\n",
    "data_kxyz_elmo.to_csv(save_name+'/'+rock_type+'_300_kxyz_results_analytical.csv', index=False)\n",
    "data_hat.to_csv(save_name+'/'+rock_type+'_300_kxyz_results_hat.csv', index=False)\n",
    "\n",
    "# Save files to predict folder\n",
    "data_kxyz_pinn.to_csv('./sim_results_predicted/'+rock_type+'_300_kxyz_results_pinn_'+mode+'.csv', index=False)\n",
    "data_kxyz_elmo.to_csv('./sim_results_predicted/'+rock_type+'_300_kxyz_results_analytical.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store summary\n",
    "data_summary_test = data_summary(Y_test_pinn*1000, Y_test*1000, 1)\n",
    "# Save results\n",
    "data_summary_test.to_csv(save_name+'/data_summary_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stat\n",
    "MARE_ana = np.mean(abs((Y_test_elmo-Y_test)/Y_test))\n",
    "MARE = np.mean(data_summary_test[\"ARE\"])\n",
    "MedRE = np.median(abs(data_summary_test[\"ARE\"]))\n",
    "ER = (MARE_ana - MARE) / MARE_ana\n",
    "# Print\n",
    "print_results(MARE_ana, MARE, MedRE, ER)\n",
    "\n",
    "with open(save_name+'/results_summary.txt', 'w') as f:\n",
    "    with redirect_stdout(f):\n",
    "        print_results(MARE_ana, MARE, MedRE, ER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LateX format for plots\n",
    "# mpl.rcParams.update({\"text.usetex\": True, \"font.family\": \"Helvetica\"})\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "\n",
    "# set plot resoluton\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "plt.rcParams['axes.linewidth'] = 1.5\n",
    "#\n",
    "# Scatter plot Log scale Train and Test sets\n",
    "#\n",
    "font_size = 16\n",
    "log_base = 10\n",
    "max = 6000\n",
    "min = 600\n",
    "color = 'mediumturquoise' #'wheat'#'lightgreen' #'mediumturquoise'\n",
    "# min = max/log_base\n",
    "x1 = data_summary_test[\"true\"]\n",
    "y1 = data_summary_test[\"hat\"]\n",
    "#\n",
    "plt.figure(figsize=(8, 6), dpi=90)\n",
    "plt.plot([min, max], [min, max], color='k', linestyle='--', dashes=(5, 3), linewidth=1.5, alpha=1)\n",
    "#\n",
    "plt.scatter(x1, y1, c=color, s=150, alpha=0.8, edgecolors='k', linewidth=0.5)\n",
    "#\n",
    "plt.plot([min*1.2, max], [min, max*0.75], color='r', linestyle='--', dashes=(5, 3), linewidth=1.5, alpha=1, label='_nolegend_')\n",
    "plt.plot([min, max*0.8], [min*1.25, max], color='r', linestyle='--', dashes=(5, 3), linewidth=1.5, alpha=1, label='_nolegend_')\n",
    "#\n",
    "plt.ylim([min, max])\n",
    "plt.xlim([min, max])\n",
    "#\n",
    "plt.tick_params(bottom=True, top=True, left=True, right=True, direction=\"in\", pad=7, length=5, width=1.5, labelsize=font_size-2, axis='both', which='both')\n",
    "plt.loglog(base=log_base)\n",
    "plt.legend(['True','Test'], loc='upper left',fontsize=font_size-2, bbox_to_anchor=(0.02, 0.95))\n",
    "plt.ylabel('Predicted permeability [mD]', size=font_size)\n",
    "plt.xlabel('True permeability [mD]', size=font_size)\n",
    "plt.title(\"\", size=font_size)\n",
    "plt.savefig(save_name+'/predictions_tt_scatter_log.png', bbox_inches='tight', dpi=400)\n",
    "plt.savefig(save_name+'/predictions_tt_scatter_log.pdf', bbox_inches='tight', dpi=400)\n",
    "plt.show()\n",
    "# plt.clf()\n",
    "## Plot histograms\n",
    "x = data_summary_test['ARE']\n",
    "color = 'skyblue'\n",
    "bins = 7\n",
    "plt.figure(figsize=(10, 6), dpi=90)\n",
    "plt.hist(x, bins=bins, color=color, alpha= 0.8, edgecolor='black', linewidth=1)\n",
    "plt.tick_params(bottom=True, top=True, left=True, right=True, direction=\"in\", pad=7, length=5, width=1.5, labelsize=font_size-2)\n",
    "plt.ylim(-0.01, 300)\n",
    "plt.xlim(-0.01, 0.35)\n",
    "MARE = np.mean(abs(x))\n",
    "MedRE = np.median(abs(x))\n",
    "plt.axvline(MARE, color='g', linestyle='dashed', linewidth=2.5)\n",
    "plt.axvline(MedRE, color='r', linestyle='dashed', linewidth=2.5)\n",
    "plt.ylabel('Number of samples [-]', size=font_size)\n",
    "plt.xlabel('Absolute relative error [-]', size=font_size)\n",
    "plt.savefig(save_name+'/ARE_hist.png', bbox_inches='tight', dpi=400)\n",
    "plt.savefig(save_name+'/ARE_hist.pdf', bbox_inches='tight', dpi=400)\n",
    "#plt.savefig('K_hist.eps', format='eps')\n",
    "plt.show()\n",
    "#plt.clf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 ('tensorflow-cpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "181f340e2df7e34e778a2366a317e676f39d4ffc23bddca1ebda0b79fed9b5e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
