{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PINN-1 code for Porous Media Permeability Upscaling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Libraries\n",
    "from numpy import sqrt\n",
    "from numpy import savetxt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import os\n",
    "# ML libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.python.keras.utils import losses_utils\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "from contextlib import redirect_stdout\n",
    "# Visualization libraries\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "# generates same random numbers each time\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "# Choose GPU to use (8 available) - skip 0 (busy)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_visible_devices(gpus[4:8], 'GPU')\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Set data type\n",
    "DTYPE='float32'\n",
    "tf.keras.backend.set_floatx(DTYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Custome Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_aug(data):\n",
    "    d = data\n",
    "    del data\n",
    "    # Augment all features\n",
    "    data_ext = pd.concat([d, d, d, d, d, d, d, d], ignore_index=True)\n",
    "    \n",
    "    # k small orders after fliping along X and Y Dirs\n",
    "    order = [[1, 2, 3, 4, 5, 6, 7, 8],\n",
    "             [2, 1, 4, 3, 6, 5, 8, 7],\n",
    "             [4, 3, 2, 1, 8, 7, 6, 5],\n",
    "             [3, 4, 1, 2, 7, 8, 5, 6]]\n",
    "    \n",
    "     # add k small orders after reversing orders along Z Dir\n",
    "    for i in range(0, len(order)):\n",
    "        order.append(order[i][::-1])\n",
    "     \n",
    "    # Transpose order array\n",
    "    order = np.array(order).T\n",
    "\n",
    "    # small k in X Directions    \n",
    "    for k in ['kx', 'ky', 'kz', 'p', 'ep']:\n",
    "        i = 1\n",
    "        for o in order:\n",
    "            # get ordered features\n",
    "            kn1 = k+str(o[0])\n",
    "            kn2 = k+str(o[1])\n",
    "            kn3 = k+str(o[2])\n",
    "            kn4 = k+str(o[3])\n",
    "            kn5 = k+str(o[4])\n",
    "            kn6 = k+str(o[5])\n",
    "            kn7 = k+str(o[6])\n",
    "            kn8 = k+str(o[7])\n",
    "            # augment small cubes features           \n",
    "            data_ext[k+str(i)] = pd.concat([d[kn1], d[kn2], d[kn3], d[kn4], \n",
    "                                            d[kn5], d[kn6], d[kn7], d[kn8]], ignore_index=True)                        \n",
    "            i = i + 1\n",
    "    \n",
    "    return data_ext\n",
    "\n",
    "def data_bal(d, LT, UT, inc, num, fill):\n",
    "\n",
    "    ran = np.arange(LT, UT, inc)\n",
    "    # intialize data frame\n",
    "    d_bal = pd.DataFrame()\n",
    "    for Lt in ran:\n",
    "        d_inc = d[(d['KX'] > Lt) & (d['KX'] <= Lt+inc)]\n",
    "        \n",
    "        if len(d_inc) > 0:\n",
    "            if len(d_inc) <= num and fill == False:\n",
    "                num_ = len(d_inc)\n",
    "            else:\n",
    "                num_ = num\n",
    "        \n",
    "            d_inc_num = d_inc.sample(n=num_, replace=fill) # True to satisfy num if num > len(d_inc_num)\n",
    "            d_bal = pd.concat([d_bal, d_inc_num], ignore_index=True)\n",
    "    \n",
    "    return d_bal\n",
    "\n",
    "def get_XY(data):\n",
    "    X = pd.DataFrame()\n",
    "    for i in range(1, 9):\n",
    "        kx = \"kx\"+str(i)\n",
    "        ky = \"ky\"+str(i)\n",
    "        kz = \"kz\"+str(i)\n",
    "        X = pd.concat([X, data[kx], data[ky], data[kz]], axis=1)\n",
    "\n",
    "    ana_sol = \"hat_\"+formula\n",
    "    Y = data[[\"KX\", ana_sol]]\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "def data_summary(hat, true, percentile):\n",
    "    # calc error\n",
    "    RE = (hat-true)/true\n",
    "    ARE = RE.abs()    \n",
    "    # Summarze data in one dataframe\n",
    "    data_summary = pd.DataFrame(columns=['hat', 'true', 'ARE'], index=range(len(true)))\n",
    "    data_summary['true'] = true.reset_index(drop=True)\n",
    "    data_summary['hat'] = hat\n",
    "    data_summary['ARE'] = ARE.reset_index(drop=True)    \n",
    "    # 95 percintile of Testing data results\n",
    "    data_summary = data_summary[(data_summary.ARE < ARE.quantile(percentile))]\n",
    "    \n",
    "    return data_summary\n",
    "    \n",
    "def print_results(RE_ana, RE, ER):\n",
    "    print('Testing MARE (Analytical) [%] = '+str(round(RE_ana*100, 1)))\n",
    "    print('Testing MARE (PINN)       [%] = '+str(round(RE*100, 1)))\n",
    "    print('Error Reduction           [%] = '+str(round(ER*100, 1)))\n",
    "    return\n",
    "\n",
    "# Visualization\n",
    "def hist(x, bins=30, color='lightgreen', alpha=1, xlabel='xlabel', title='title',\n",
    "         stat=None, xlim=None, ylim=None, version=''):\n",
    "    MARE = np.mean(abs(x))\n",
    "    MedRE = np.median(abs(x))\n",
    "    plt.hist(x, bins=bins, color=color, alpha=alpha, edgecolor='black', linewidth=0.7)\n",
    "    \n",
    "    plt.tick_params(direction=\"in\", pad=5, labelsize=12)\n",
    "    if ylim != None:\n",
    "        plt.ylim(ylim[0],ylim[1])\n",
    "    if xlim != None:\n",
    "        plt.xlim(xlim[0],xlim[1])\n",
    "    if stat == False:\n",
    "        plt.title(title, size=12)\n",
    "    else:\n",
    "        MARE = np.mean(abs(x))\n",
    "        MedRE = np.median(abs(x))\n",
    "        plt.axvline(MARE, color='r', linestyle='dashed', linewidth=1)\n",
    "        plt.axvline(MedRE, color='k', linestyle='dashed', linewidth=1)\n",
    "        plt.legend(['Mean', 'Median'], loc='upper right')\n",
    "        plt.title(title+'\\n MARE= '+str(round(MARE, 2)), size=12)\n",
    "        # plt.title(title, size=12)\n",
    "    plt.ylabel('Number of samples [-]', size=12)\n",
    "    plt.xlabel(xlabel, size=12)\n",
    "    plt.savefig(color+version+'_hist.png', dpi=300)\n",
    "    plt.savefig(color+version+'_hist.pdf', dpi=300)\n",
    "    #plt.savefig('K_hist.eps', format='eps')\n",
    "    plt.show()\n",
    "    #plt.clf()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data - Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data filename\n",
    "# data_file = \"./data/BrSS_compiled_data_upscaled\" \n",
    "data_file = \"./data/data_3_rocks_raw\" \n",
    "#data_file = \"./data/data_3_rocks_bal\"                       #update\n",
    "# Save Folder\n",
    "formula = 'elmorsy'\n",
    "trial = '_3_rocks_KF_'+formula+'_8x4L_CT0.25_CA0.75_save_all'                   #update\n",
    "data_split = 0.2                                            #update\n",
    "C_true = 0.25                                               #update \n",
    "C_ana  = 0.75                                               #update\n",
    "ana_sol = \"hat_\"+formula\n",
    "save_name = './networks/mlp_'+'data_split_'+str(data_split)+trial\n",
    "# Load data files\n",
    "data = read_csv(data_file+\".csv\").astype('float32')\n",
    "# List data heads\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization and Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce data\n",
    "LT = 1000\n",
    "UT = 5000\n",
    "inc = 100\n",
    "num = 120\n",
    "data_red = data[(data['KX']*1000 > LT) & (data['KX']*1000 <= UT)]\n",
    "\n",
    "# Plot Histogram\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "# set plot resoluton\n",
    "plt.rcParams[\"figure.dpi\"] = 90\n",
    "#\n",
    "# plot histograms\n",
    "'''\n",
    "hist(data_red['KX']*1000, bins=np.arange(LT, UT, inc), color='seashell', alpha=1, \n",
    "     xlabel='Permeability [mD]', title='Original Dataset Permeability Histogram', \n",
    "     stat=False, ylim=None, version='_orig')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data_red.sample(frac=0.2, random_state=1)\n",
    "\n",
    "# List data heads\n",
    "# display(data_test.head())\n",
    "\n",
    "data_train = data_red.drop(data_test.index)\n",
    "# display(data_train.head())\n",
    "\n",
    "'''\n",
    "# plot histograms\n",
    "hist(data_test['KX']*1000, bins=np.arange(LT, UT, inc), color='seashell', alpha=1, \n",
    "     xlabel='Permeability [mD]', title='Test Set Permeability Histogram', \n",
    "     stat=False, ylim=None, version='_test')\n",
    "\n",
    "# plot histograms\n",
    "hist(data_train['KX']*1000, bins=np.arange(LT, UT, inc), color='seashell', alpha=1, \n",
    "     xlabel='Permeability [mD]', title='Train Set Permeability Histogram', \n",
    "     stat=False, ylim=None, version='_train')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Proccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment data\n",
    "data_train_aug = data_aug(data_train)\n",
    "\n",
    "# Balance data\n",
    "data_train_bal = data_bal(data_train_aug, LT=LT/1000, UT=UT/1000, inc=inc/1000, num=num, fill=True)\n",
    "'''\n",
    "# plot histograms\n",
    "hist(data_train_aug['KX']*1000, bins=np.arange(LT, UT, inc), color='seashell', alpha=1, \n",
    "     xlabel='Permeability [mD]', title='Augmented Train Set Permeability Histogram', \n",
    "     stat=False, ylim=[0, 1200], version='_test_aug')\n",
    "\n",
    "# plot histograms\n",
    "hist(data_train_bal['KX']*1000, bins=np.arange(LT, UT, inc), color='seashell', alpha=1, \n",
    "     xlabel='Permeability [mD]', title='Balanced Train Set Permeability Histogram', \n",
    "     stat=False, ylim=[0, num*1.2], version='_train_bal')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select Input and Target data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = get_XY(data_test)\n",
    "X_train, Y_train = get_XY(data_train_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into Train and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test_0, Y_train, Y_test_0 = train_test_split(X_train, Y_train, train_size=0.9999, random_state=1)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size= 0.2, random_state=1)\n",
    "\n",
    "Y_train_true = Y_train[\"KX\"]\n",
    "Y_val_true = Y_val[\"KX\"]\n",
    "Y_test_true = Y_test[\"KX\"]\n",
    "\n",
    "Y_train_phy = Y_train[ana_sol]\n",
    "Y_val_phy = Y_val[ana_sol]\n",
    "Y_test_phy = Y_test[ana_sol]\n",
    "\n",
    "Y_train_tp = (C_true * Y_train_true) + (C_ana * Y_train_phy)\n",
    "Y_val_tp = (C_true * Y_val_true) + (C_ana * Y_val_phy)\n",
    "Y_test_tp = (C_true * Y_test_true) + (C_ana * Y_test_phy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PINN Custome Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model():\n",
    "    # Initialize a feedforward neural network\n",
    "    n_features = X_train.shape[1]\n",
    "    inputs = keras.Input(shape=(n_features))\n",
    "    x = layers.Dense((8), activation='relu', name='1st_layer')(inputs)  # 'name=' is optional\n",
    "    # x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense((8), activation='relu', name='2nd_layer')(x)\n",
    "    # x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense((8), activation='relu', name='3nd_layer')(x)\n",
    "    # x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense((8), activation='relu', name='4th_layer')(x)\n",
    "    # x = layers.Dropout(0.2)(x)\n",
    "    # x = layers.Dense((8), activation='relu', name='5th_layer')(x)\n",
    "    # x = layers.Dense((8), activation='relu', name='6th_layer')(x)\n",
    "    # x = layers.Dense((8), activation='relu', name='7th_layer')(x)\n",
    "    # x = layers.Dense((8), activation='relu', name='8th_layer')(x)\n",
    "    outputs = layers.Dense((1))(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "# Loss Function\n",
    "loss = tf.keras.losses.MeanAbsolutePercentageError(\n",
    "        reduction=losses_utils.ReductionV2.AUTO, name='mean_absolute_percentage_error')\n",
    "acc_metric = tf.keras.metrics.MeanAbsolutePercentageError()\n",
    "# Callbacks\n",
    "csv_logger = CSVLogger(save_name+'/training.log')\n",
    "\n",
    "checkpoint_filepath = save_name+'/checkpoint'\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "                            filepath=checkpoint_filepath,\n",
    "                            save_weights_only=True,\n",
    "                            monitor='val_loss',\n",
    "                            mode='min',\n",
    "                            save_best_only=True)\n",
    "\n",
    "callbacks = [csv_logger, model_checkpoint_callback]\n",
    "\n",
    "# Create Diectory\n",
    "isExist = os.path.exists(save_name)\n",
    "if not isExist:\n",
    "     os.mkdir(save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "BATCH_SIZE = 16\n",
    "# Using Multiple GPUs takes more time than using one GPU.\n",
    "model = init_model()        \n",
    "# Compile Model\n",
    "model.compile(loss=loss, \n",
    "                optimizer=optimizer, \n",
    "                metrics=[acc_metric])\n",
    "# Fit Model\n",
    "history = model.fit(\n",
    "          X_train,\n",
    "          Y_train_tp,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=num_epochs,\n",
    "          validation_data=(X_val, Y_val_tp),\n",
    "          callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save model and load weights\n",
    "tf.saved_model.save(model, save_name)\n",
    "np.save(save_name+'/history.npy', history.history)\n",
    "model.load_weights(checkpoint_filepath)\n",
    "\n",
    "with open(save_name+'/model_summary.txt', 'w') as f:\n",
    "    with redirect_stdout(f):\n",
    "        model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Predictions and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "Y_test_hat = tf.squeeze(model.predict(X_test)).numpy()\n",
    "Y_train_hat = tf.squeeze(model.predict(X_train)).numpy()\n",
    "# Store summary\n",
    "data_summary_test = data_summary(Y_test_hat*1000, Y_test[\"KX\"]*1000, 0.95)\n",
    "data_summary_train = data_summary(Y_train_hat*1000, Y_train[\"KX\"]*1000, 0.90)\n",
    "# Save results\n",
    "data_summary_test.to_csv(save_name+'/data_summary_test.csv', index=False)\n",
    "data_summary_train.to_csv(save_name+'/data_summary_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stat\n",
    "RE_ana = np.mean(abs((Y_test[ana_sol]-Y_test[\"KX\"])/Y_test[\"KX\"]))\n",
    "RE = np.mean(data_summary_test[\"ARE\"])\n",
    "ER = (RE_ana - RE) / RE_ana\n",
    "# Print\n",
    "print_results(RE_ana, RE, ER)\n",
    "\n",
    "with open(save_name+'/results_summary.txt', 'w') as f:\n",
    "    with redirect_stdout(f):\n",
    "        print_results(RE_ana, RE, ER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LateX format for plots\n",
    "# mpl.rcParams.update({\"text.usetex\": True, \"font.family\": \"Helvetica\"})\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "\n",
    "# set plot resoluton\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "plt.rcParams['axes.linewidth'] = 1.5\n",
    "#\n",
    "# Scatter plot Log scale Train and Test sets\n",
    "#\n",
    "font_size = 16\n",
    "log_base = 10\n",
    "max = 6500\n",
    "min = max/log_base\n",
    "x1 = data_summary_train[\"true\"]\n",
    "y1 = data_summary_train[\"hat\"]\n",
    "x2 = data_summary_test[\"true\"]\n",
    "y2 = data_summary_test[\"hat\"]\n",
    "#\n",
    "plt.figure(figsize=(8, 6), dpi=90)\n",
    "plt.plot([min, max], [min, max], color='k', linestyle='--', dashes=(5, 3), linewidth=1.5, alpha=1)\n",
    "#\n",
    "plt.scatter(x1, y1, c='silver', s=50, alpha=0.8, edgecolors='k', linewidth=0.5)\n",
    "plt.scatter(x2, y2, c='lightgreen', s=50, alpha=0.8, edgecolors='g', linewidth=0.5)\n",
    "#\n",
    "plt.ylim([min, max])\n",
    "plt.xlim([min, max])\n",
    "#\n",
    "plt.tick_params(bottom=True, top=True, left=True, right=True, direction=\"in\", pad=7, length=5, width=1.5, labelsize=font_size-2, axis='both', which='both')\n",
    "plt.loglog(base=log_base)\n",
    "plt.legend(['True', 'Train', 'Test'], loc='upper left',fontsize=font_size-2, bbox_to_anchor=(0.02, 0.98))\n",
    "plt.ylabel('Predicted permeability [mD]', size=font_size)\n",
    "plt.xlabel('True permeability [mD]', size=font_size)\n",
    "plt.title(\"\", size=font_size)\n",
    "plt.savefig(save_name+'/predictions_tt_scatter_log.png', bbox_inches='tight', dpi=400)\n",
    "plt.savefig(save_name+'/predictions_tt_scatter_log.pdf', bbox_inches='tight', dpi=400)\n",
    "plt.show()\n",
    "# plt.clf()\n",
    "## Plot histograms\n",
    "x = data_summary_test['ARE']\n",
    "color = 'skyblue'\n",
    "bins = 20\n",
    "plt.figure(figsize=(10, 6), dpi=90)\n",
    "plt.hist(x, bins=bins, color=color, alpha= 0.8, edgecolor='black', linewidth=1)\n",
    "plt.tick_params(bottom=True, top=True, left=True, right=True, direction=\"in\", pad=7, length=5, width=1.5, labelsize=font_size-2)\n",
    "plt.ylim(-0.01, 50)\n",
    "plt.xlim(-0.01, 0.5)\n",
    "MARE = np.mean(abs(x))\n",
    "MedRE = np.median(abs(x))\n",
    "plt.axvline(MARE, color='g', linestyle='dashed', linewidth=2.5)\n",
    "plt.axvline(MedRE, color='r', linestyle='dashed', linewidth=2.5)\n",
    "plt.ylabel('Number of samples [-]', size=font_size)\n",
    "plt.xlabel('Absolute relative error [-]', size=font_size)\n",
    "plt.savefig(save_name+'/ARE_hist.png', bbox_inches='tight', dpi=400)\n",
    "plt.savefig(save_name+'/ARE_hist.pdf', bbox_inches='tight', dpi=400)\n",
    "#plt.savefig('K_hist.eps', format='eps')\n",
    "plt.show()\n",
    "#plt.clf()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c1dd7de172b9d0b9261d13ab63e40c75ab95161a91d78f9c3e0b88a262714b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
